{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite Database for Southern Ocean Phytoplankton Data\n",
    "## Sample Code\n",
    "### TODO: Add sophy.ipynb code here\n",
    "\n",
    "## Data transformations\n",
    "### TODO: Add dataformat.ipynb code here\n",
    "\n",
    "## Notes\n",
    "### Complex data transformation functions\n",
    "- FINISH SECTION; INCLUDE CODE SNIPPETS\n",
    "- Can combine a common format of phytoplankton taxa data into our format\n",
    "- Includes row averaging system, multi-key joins, and JSON compression\n",
    "\n",
    "\n",
    "Purpose: Starting point for sqlite observational datasets. give example features/how to build your database, example advanced queries that can be done, examples of how the results can be plotted\n",
    "- Advanced queries with SQL and plotting with python\n",
    "- Rather than doing a bunch of data transformations in pandas, you can use the existing, quick, platform of sqlite\n",
    "- With Jupyter, database is a command line like interface that allows for lots of experimentation\n",
    "- Can help you create a relational database with a cell in jupyter that generates your current schema (eralchemy package?). Then you can experiment with create_tables.sql and see the visual right away\n",
    "\n",
    "### Updates on new table microscopy in schema and DB design\n",
    "- FINISH SECTION\n",
    "- Some datasets provide microscopy data with many columns of taxa information that were all measured in the same sample. There is one row in the sample table that they match to so I needed another table to avoid storing these taxa redundantly in the sample table.\n",
    "- Microscopy table allows for many taxa to be grouped under the same sample with a foreign key\n",
    "\n",
    "\n",
    "### Fall 2022 goals\n",
    "SOPhy database\n",
    "- Continue adding datasets from papers\n",
    "  - Add phytoplankton groups feature and modify schema to accommodate variety of data\n",
    "- Make frontend for visualizations and data use\n",
    "  - GitHub pages, Jupyter Notebook, or Python to visualization packages\n",
    "- Publish database\n",
    "  - Hosted on GitHub or Zenodo repository\n",
    "  - Publish paper\n",
    "\n",
    "### LTER ML model focus\n",
    "- The LTER dataset is the golden standard in terms of format and amount of data (30,000 data points over 30 years)\n",
    "- The rest of the datasets will contain ~150 data points total and seem insufficient to train the niche model\n",
    "- Idea: Make the niche model a proof of concept using only the LTER dataset\n",
    "  - Continue with original plan of clustering using BGC-ARGO floats\n",
    "  - Then instead of training niche model on the entire dataset just use LTER\n",
    "  - If successful, we will have a proof of concept for the core idea\n",
    "  - Once NASA PACE is online we will have significantly more data and can apply the same techniques to all available data\n",
    "\n",
    "### Data format challenge\n",
    "- I have started working through the datasets that can be entered in the database\n",
    "- The issue is that their formats are significantly different\n",
    "  - Some contain ratios of values where we expect a quantity\n",
    "  - Some contain a mix of CHEMTAX and microscopy in one row\n",
    "  - Some average values over a certain region and don't publish the individual points, so we cannot use it\n",
    "  - In general, many essential fields are missing (lat, depth) that need to be tracked down\n",
    "- We will have to come up with a new approach to entering data, since there are oceanography decisions to be made when entering data where I don't have experience.\n",
    "\n",
    "\n",
    "### Data Science Notebook Options\n",
    "#### Jupyter\n",
    "- Runs locally so no internet connection needed after download\n",
    "- Dataset has to be stored on local machine\n",
    "#### Google Colab\n",
    "- Ease of use to share and use\n",
    "- Backend performance of Google servers\n",
    "  - Particularly important for ML models which actually need extra power\n",
    "- Modules are pre-installed\n",
    "- Easy versioning with Google and GitHub integration\n",
    "#### Deepnote\n",
    "- Designed for collaboration. Multiple people can live edit a notebook\n",
    "- Many code \"intelligence\" features like auto install packages and autocomplete\n",
    "\n",
    "Current pick is to create Jupyter file and work in Intellij. Then if storage/performance becomes an issue, we can easily port the document into Google Colab. UW provides Google Drive storage which could be an option to store the database and then connect to Colab. If the database is stored on GitHub (regular Jupyter Notebook), file size may become an issue.\n",
    "\n",
    "\n",
    "### Summer 2022 Goals\n",
    "- Add more data to SOPHY\n",
    "  - Start with LTER and Phytobase. LTER is the most ideal format to test and Phytobase has by far the most microscopy data\n",
    "- Define regions and tags for data\n",
    "  - Use GeoJSON/GeoPandas to label data\n",
    "  - Brainstorm all the regions that are needed (oceans, fronts, etc.)\n",
    "- Create Jupyter notebook or Google Colab front end\n",
    "  - Custom python functions for common oceanography use cases of this data\n",
    "  - Direct SQLite editor for advanced users\n",
    "  - Map visualization of all data\n",
    "  - UW Research Computing Club has access to Azure, AWS, etc. servers for ML \n",
    "- Start niche model/ML/Objective 3\n",
    "  - An idea is to start with unsupervised learning and then switching to supervised. Lets the ML project get started and potentially find trends we were not expecting.\n",
    "  - Could use synthetic data while real data is not there \n",
    "\n",
    "### Feature Ideas\n",
    "- Think about how this could be adapted to the future of phytoplankton data\n",
    "  - For example if Argo floats may start having taxa data\n",
    "  - Or just any sensor based data that includes taxa because our current model is just for _in situ_ observations\n",
    "- What are side 'features/products' that can be made for SOPHY and general use\n",
    "  - Adapter class that provides starter code for converting dataset into SQLite\n",
    "\n",
    "### Spring Quarter 2022 Wrap-Up\n",
    "- Feature List:\n",
    "  - Main script can add two full data to database (~1.3 mil rows)\n",
    "    - _LTER_ and _Phytobase_ were chosen since they cover the two different formats of taxonomy data (microscopy and CHEMTAX)\n",
    "    - Also adds foreign keys from microscopy table (Phytobase)\n",
    "  - Query WoRMS database for taxa records\n",
    "    - Converts result from pyworms package into formatted dataframe\n",
    "  - **Modular code** that easily translates to other data\n",
    "    - Code from this quarter is quite short since I have reviewed each operation several times to prioritize simplicity and performance as a setup for the summer\n",
    "    - The formula is made for how to add new data; data read, modify, write, and fk setup\n",
    "- Next steps:\n",
    "  - Support for location, source, and tag tables as part of data insert\n",
    "  - Work on test suite even though code format may change (new classes/functions)\n",
    "  - Build collection of data we want to add to SOPHY\n",
    "    - Can be done concurrently with writing new code\n",
    "    - The schema and code will adapt to deal with new challenges or design changes\n",
    "  - How to interact with the database?\n",
    "    - Research how scientific databases are usually presented (GUI, Jupyter, etc.)\n",
    "    - We can make pre-built use cases for common operations\n",
    "\n",
    "### Iterating over a Dataframe\n",
    "- I was finding methods to iterate over a dataframe such as itterows(), but operations seemed to be taking longer than expected (~2s)\n",
    "- Across several functions, I was incorrectly iterating over a dataframe rather than **vectorization** and using pandas built in indexing\n",
    "- **Vectorization** means using pre-defined, highly optimized methods to process data\n",
    "- The time complexity and runtime is orders of magnitude lower when using vectorization since DataFrame's are meant to use it\n",
    "\n",
    "### Natural Key vs AphiaID\n",
    "- Datasets that contain microscopy data will most likely contain a 'scientific_name' field that we will also have a corresponding AphiaID we will get from the WoRMS database\n",
    "- sci_name is a key for aphia_id and vice versa, so we should only store one of them in the main table\n",
    "- However not all microscopy data goes to the level of sci_name and may only go down to genus\n",
    "- In that case the aphia_id approach is correct since we can keep the aphia_id for the genus and then have access to it's full taxa through the microscopy table whereas that would not be possible with only a sci_name col in the main table\n",
    "- This approach also correctly handles the issue of when two names are different in Python or just visually, but actually represent the same species (WoRMS flags species name as unaccepted)\n",
    "```python\n",
    "import pyworms\n",
    "pyworms.aphiaRecordsByMatchNames('Coccopterum labyrinthus')  # accepted name\n",
    "pyworms.aphiaRecordsByMatchNames('Coccopterum_labyrinthus')  # slightly different formatting\n",
    "pyworms.aphiaRecordsByMatchNames('Pterosperma labyrinthus')  # unaccepted name, but same species\n",
    "```\n",
    "- A rank column may also be useful however it seems like data that can be inferred from what data is present and not present, so TBD\n",
    "\n",
    "### Workflow of Python and SQLite\n",
    "- Both have a way of storing large amounts of 2D data(dataframes vs. tables) so it can be confusing when to use which\n",
    "- Dataframes are much easier to modify, can be used alongside python code, and keeps history of operation\n",
    "- SQLite is [much faster](https://www.thedataincubator.com/blog/2018/05/23/sqlite-vs-pandas-performance-benchmarks/) at select style operations\n",
    "- Data loading, cleanup, and write to database will be done in Python with dataframes \n",
    "- Then queries only with SQLite\n",
    "\n",
    "### Re-normalization of columns: major fix\n",
    "- Updated schema to avoid normalizing much more than needed (3 FK's in schema)\n",
    "- I used to create a whole new cruise table since I thought repeating a string in the same column was repeated data that needed to be fixed\n",
    "- **Normalization means when one column can allow you to infer other columns**\n",
    "  - For example when knowing the source name (LTER) can tell you author, doi, url, etc. and that data doesn't need to be repeated across many columns\n",
    "  - One **key** gives you access to the other columns\n",
    "  - Then that data goes in a new table and the main table simply references that one key to have access to the rest of the data\n",
    "  - It's ok to repeat values within a column, but there shouldn’t be columns that infer other columns within a table\n",
    "- This change has drastically reduced the number of tables in the DB (see schema v5 &rarr; v6)\n",
    "\n",
    "### Primary Key Setup\n",
    "- They can be auto-generated in SQLite, but is that the best way?\n",
    "```sql\n",
    "id int primary key autoincrement\n",
    "```\n",
    "- Some alternatives are GUID, UUID, and natural keys\n",
    "- The key does not need to be unique across the database so GUID is probably overkill\n",
    "- Autoincrement is the best option for now since it also the default option in SQLite\n",
    "\n",
    "### Tech Stack\n",
    "- Dataset(CSV) &rarr; DataFrame(Python) &rarr; sophy(SQLite)\n",
    "- Sophy can be created in sqlite (createTables.sql) then saved as an .db file to insert data with Python\n",
    "- First test is to load the LTER dataset through the tech stack\n",
    "\n",
    "### Column that can hold id of two possible tables\n",
    "- The problem is called Polymorphic Associations\n",
    "- I would like to reference two tables to the same column with an FK\n",
    "  - For example microscopy id OR taxa id\n",
    "- One fix is to make a supertable that generates a primary key, then the two 'sub' tables reference their id from the super table\n",
    "- This is essentially a PFK (primary foreign key) since each row in the sub tables is unique but technically still a foreign key\n",
    "- The only issue is that it complicates the schema and doing queries since there is an extra table needed to do inserts\n",
    "\n",
    "### Phytoplankton microscopy vs. pigment based taxa classification\n",
    "- I originally thought  each (in situ) dataset would provide a species name that could be converted into a taxonomy tree (using microscopy)\n",
    "- In reality they provide either microscopy OR pigment data. Both provide a classification of the taxonomy of each sample\n",
    "- Pigments are the chemical method by using pigment markers and the CHEMTAX software to determine the species composition of a water sample. CHEMTAX gives the percentage of ~5 groups to determine species comp.\n",
    "- The schema needs to be restructured to reflect this change. One row in one dataset represents a full water sample(pigments) but one row in another dataset represents the amount of each species found (microscopy). Since they are inherently different row structures, a new table may be needed.\n",
    "\n",
    "### Tags\n",
    "- We need to be able to classify rows with certain data like the method of sampling, quality of data, etc\n",
    "- Tags are a great way to do this since we can create a tag, then reuse that same tag for several rows\n",
    "- Represented by a many &rarr; many relationship in SQL.\n",
    "\n",
    "### WoRMS Database\n",
    "- We will be using [WoRMS](https://www.marinespecies.org/) since it is the most relevant and established database for our research\n",
    "- All taxonomy data will be sourced from there\n",
    "- There is Python package that allows us to query the DB and get the full taxa of a species\n",
    "```python\n",
    "import pyworms\n",
    "pyworms.aphiaRecordsByMatchNames('Carteria marina')\n",
    "```\n",
    "\n",
    "### Database Planning ###\n",
    "- Who are the users?\n",
    "  - Researchers (oceanography most likely)\n",
    "  - They have experience with Python and Jupyter notebooks\n",
    "  - Should make a document on how to use the program\n",
    "- RDMS vs. Excel/CSV\n",
    "  - CSV will get slow in the range of millions of columns to even just search\n",
    "  - Major benefits will be seen when making the niche model\n",
    "    - Researchers may not have super complex queries but the niche model will\n",
    "    - The model will be **significantly** faster at runtime (visualization)\n",
    "  - Data is also much easier to add since structure is clearly defined"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
